{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO7jTfzWS2bjoswqbcVmoub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwh133/Transformer/blob/main/%E7%AC%AC6%E7%AB%A0_%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pre-Processing datasets for Machine Translation\n",
        "* Copyright 2020, Denis Rothman, MIT License\n",
        "* Denis Rothman modified the code for educational purposes.\n",
        "#Reference:\n",
        "* Jason Brownlee PhD, ‘How to Prepare a French-to-English Dataset for Machine Translation\n",
        "* https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/\n"
      ],
      "metadata": {
        "id": "IfNMpZ6cErON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据预处理"
      ],
      "metadata": {
        "id": "ld2eyCAoQGQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c6e1lpu8FBUQ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pickle import dump"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "metadata": {
        "id": "PqfogL1DFBHP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')"
      ],
      "metadata": {
        "id": "tRi0F6t6Hw56"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "\tlengths = [len(s.split()) for s in sentences]\n",
        "\treturn min(lengths), max(lengths)"
      ],
      "metadata": {
        "id": "nlbGuyuOH1rP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean lines\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering 正则表达式\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\tprint(re_print)\n",
        "\tprint('----------')\n",
        "\t# prepare translation table for removing punctuation 去除标点符号\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tprint(table)\n",
        "\tfor line in lines:\n",
        "\t\t# normalize unicode characters\n",
        "\t\tline = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case 转换为小写\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token 删除标点符号\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# remove non-printable chars form each token 删除不可打印字符\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# remove tokens with numbers in them 删除包含数字的标记\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\treturn cleaned"
      ],
      "metadata": {
        "id": "lRyq49cOH30w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "欧洲议会会议平行语料库（法语-英语数据集）[链接](https://www.statmt.org/europarl/v7/fr-en.tgz)"
      ],
      "metadata": {
        "id": "60-QP9DFIN7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'English.pkl'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(cleanf,outfile)\n",
        "outfile.close()\n",
        "print(filename,\" saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1rNXHNlH50j",
        "outputId": "715b4131-d11b-4304-c714-8860ba77ff35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English data: sentences=2007723, min=0, max=668\n",
            "re.compile('[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~\\\\ \\\\\\t\\\\\\n\\\\\\r\\\\\\x0b\\\\\\x0c]')\n",
            "----------\n",
            "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
            "English.pkl  saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "cleanf=clean_lines(sentences)\n",
        "filename = 'French.pkl'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(cleanf,outfile)\n",
        "outfile.close()\n",
        "print(filename,\" saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptNfvG9VIAbc",
        "outputId": "a4713d25-59a1-4d3c-c651-8d4fffce163e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French data: sentences=2007723, min=0, max=693\n",
            "re.compile('[^0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~\\\\ \\\\\\t\\\\\\n\\\\\\r\\\\\\x0b\\\\\\x0c]')\n",
            "----------\n",
            "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 39: None, 40: None, 41: None, 42: None, 43: None, 44: None, 45: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None}\n",
            "French.pkl  saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from collections import Counter\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        ""
      ],
      "metadata": {
        "id": "xVcZmmIbM1On"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a frequency table for all words 词汇计数器\n",
        "def to_vocab(lines):\n",
        "\tvocab = Counter()\n",
        "\tfor line in lines:\n",
        "\t\ttokens = line.split()\n",
        "\t\tvocab.update(tokens)\n",
        "\treturn vocab\n",
        "\n",
        "# remove all words with a frequency below a threshold\n",
        "def trim_vocab(vocab, min_occurance):\n",
        "\ttokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "\treturn set(tokens)"
      ],
      "metadata": {
        "id": "39xh4svpM14F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mark all OOV with \"unk\" for all lines\n",
        "def update_dataset(lines, vocab):\n",
        "\tnew_lines = list()\n",
        "\tfor line in lines:\n",
        "\t\tnew_tokens = list()\n",
        "\t\tfor token in line.split():\n",
        "\t\t\tif token in vocab:\n",
        "\t\t\t\tnew_tokens.append(token)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_tokens.append('unk')\n",
        "\t\tnew_line = ' '.join(new_tokens)\n",
        "\t\tnew_lines.append(new_line)\n",
        "\treturn new_lines"
      ],
      "metadata": {
        "id": "aoeuRjmxNjP1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load English dataset\n",
        "filename = 'English.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('English Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New English Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'english_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "\tprint(\"line\",i,\":\",lines[i])\n",
        "\n",
        "# load French dataset\n",
        "filename = 'French.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('French Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New French Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'french_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(20):\n",
        "\tprint(\"line\",i,\":\",lines[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chbyQQ69NoIK",
        "outputId": "7cfa580d-bac9-4541-e633-fb8a261af9d8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary: 105357\n",
            "New English Vocabulary: 41746\n",
            "Saved: english_vocab.pkl\n",
            "line 0 : resumption of the session\n",
            "line 1 : i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
            "line 2 : although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
            "line 3 : you have requested a debate on this subject in the course of the next few days during this partsession\n",
            "line 4 : in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n",
            "line 5 : please rise then for this minute s silence\n",
            "line 6 : the house rose and observed a minute s silence\n",
            "line 7 : madam president on a point of order\n",
            "line 8 : you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n",
            "line 9 : one of the people assassinated very recently in sri lanka was mr unk unk who had visited the european parliament just a few months ago\n",
            "line 10 : would it be appropriate for you madam president to write a letter to the sri lankan president expressing parliaments regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation\n",
            "line 11 : yes mr evans i feel an initiative of the type you have just suggested would be entirely appropriate\n",
            "line 12 : if the house agrees i shall do as mr evans has suggested\n",
            "line 13 : madam president on a point of order\n",
            "line 14 : i would like your advice about rule concerning inadmissibility\n",
            "line 15 : my question relates to something that will come up on thursday and which i will then raise again\n",
            "line 16 : the cunha report on multiannual guidance programmes comes before parliament on thursday and contains a proposal in paragraph that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually\n",
            "line 17 : it says that this should be done despite the principle of relative stability\n",
            "line 18 : i believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible\n",
            "line 19 : i want to know whether one can raise an objection of that kind to what is merely a report not a legislative proposal and whether that is something i can competently do on thursday\n",
            "French Vocabulary: 141642\n",
            "New French Vocabulary: 58800\n",
            "Saved: french_vocab.pkl\n",
            "line 0 : reprise de la session\n",
            "line 1 : je declare reprise la session du parlement europeen qui avait ete interrompue le vendredi decembre dernier et je vous renouvelle tous mes vux en esperant que vous avez passe de bonnes vacances\n",
            "line 2 : comme vous avez pu le constater le grand bogue de lan ne sest pas produit en revanche les citoyens dun certain nombre de nos pays ont ete victimes de catastrophes naturelles qui ont vraiment ete terribles\n",
            "line 3 : vous avez souhaite un debat a ce sujet dans les prochains jours au cours de cette periode de session\n",
            "line 4 : en attendant je souhaiterais comme un certain nombre de collegues me lont demande que nous observions une minute de silence pour toutes les victimes des tempetes notamment dans les differents pays de lunion europeenne qui ont ete touches\n",
            "line 5 : je vous invite a vous lever pour cette minute de silence\n",
            "line 6 : le parlement debout observe une minute de silence\n",
            "line 7 : madame la presidente cest une motion de procedure\n",
            "line 8 : vous avez probablement appris par la presse et par la television que plusieurs attentats a la bombe et crimes ont ete perpetres au sri lanka\n",
            "line 9 : lune des personnes qui vient detre assassinee au sri lanka est m unk unk qui avait rendu visite au parlement europeen il y a quelques mois a peine\n",
            "line 10 : ne pensezvous pas madame la presidente quil conviendrait decrire une lettre au president du sri lanka pour lui communiquer que le parlement deplore les morts violentes dont celle de m unk et pour linviter instamment a faire tout ce qui est en son pouvoir pour chercher une reconciliation pacifique et mettre un terme a cette situation particulierement difficile\n",
            "line 11 : oui monsieur evans je pense quune initiative dans le sens que vous venez de suggerer serait tout a fait appropriee\n",
            "line 12 : si lassemblee en est daccord je ferai comme m evans la suggere\n",
            "line 13 : madame la presidente cest une motion de procedure\n",
            "line 14 : je voudrais vous demander un conseil au sujet de larticle qui concerne lirrecevabilite\n",
            "line 15 : ma question porte sur un sujet qui est a lordre du jour du jeudi et que je souleverai donc une nouvelle fois\n",
            "line 16 : le paragraphe du rapport cunha sur les programmes dorientation pluriannuels qui sera soumis au parlement ce jeudi propose dintroduire des sanctions applicables aux pays qui ne respectent pas les objectifs annuels de reduction de leur flotte\n",
            "line 17 : il precise que cela devrait etre fait malgre le principe de stabilite relative\n",
            "line 18 : a mon sens le principe de stabilite relative est un principe juridique fondamental de la politique commune de la peche et toute proposition le bouleversant serait juridiquement irrecevable\n",
            "line 19 : je voudrais savoir si lon peut avancer une objection de ce type a ce qui nest quun rapport pas une proposition legislative et si je suis habilite a le faire ce jeudi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此，数据预处理已经介绍完毕，可以把数据集提供给transformer进行训练"
      ],
      "metadata": {
        "id": "N4slBc-uN85_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 用BLEU（双语评估协作分数）评估机器翻译\n",
        "使用自然语言工具包NLTK（natural language toolkit）来实现BLEU"
      ],
      "metadata": {
        "id": "Cek0CX4TN2xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU : Bilingual Evaluation Understudy Score\n",
        "#Copyright 2020, MIT License BLEU Examples\n",
        "#REF PAPER: Kishore Papineni, et al.,2002,“BLEU: a Method for Automatic Evaluation of Machine Translation“.\n",
        "#                                                https://www.aclweb.org/anthology/P02-1040.pdf\n",
        "#NLTK : Natural Language Toolkit\n",
        "#NLTK sentence_bleu doc: http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu\n",
        "#NLTK smoothing doc: https://www.nltk.org/api/nltk.translate.html\n",
        "#NLTK REF PAPER for smoothing():Chen et al.,http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
        "#REF DOC  : https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "metadata": {
        "id": "2mQXwv3QODw-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1\n",
        "reference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']]\n",
        "candidate = ['the', 'cat', 'likes', 'milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Example 1', score)\n",
        "\n",
        "#Example 2\n",
        "reference = [['the', 'cat', 'likes', 'milk']]\n",
        "candidate = ['the', 'cat', 'likes', 'milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Example 2', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THxDyPkNOjtu",
        "outputId": "3bb2fadf-cb29-47f8-d50f-49f8a56381f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 1.0\n",
            "Example 2 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3\n",
        "reference = [['the', 'cat', 'likes', 'milk']]\n",
        "candidate = ['the', 'cat', 'enjoys','milk']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Example 3', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyKxbpX-Ooce",
        "outputId": "78239fda-eda1-48cb-f407-cc3767fa0681"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 3 1.0547686614863434e-154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 标注平滑\n",
        "chencherry：通过引入不确定性使得模型能够更加开放地对待未来的变化和转换"
      ],
      "metadata": {
        "id": "KtTAf_zPPoUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 4\n",
        "reference = [['je','vous','invite', 'a', 'vous', 'lever','pour', 'cette', 'minute', 'de', 'silence']]\n",
        "candidate = ['levez','vous','svp','pour', 'cette', 'minute', 'de', 'silence']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"without soothing score\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0SSU3DwO5-z",
        "outputId": "e9a4f00f-69a3-4eff-f0db-9d714d16e262"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without soothing score 0.37188004246466494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "往评估中添加一些开放的平滑，分数大幅提高"
      ],
      "metadata": {
        "id": "G23T_SE6P9M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chencherry = SmoothingFunction()\n",
        "r1=list('je vous invite a vous lever pour cette minute de silence')\n",
        "candidate=list('levez vous svp pour cette minute de silence')\n",
        "\n",
        "#sentence_bleu([reference1, reference2, reference3], hypothesis2,smoothing_function=chencherry.method1)\n",
        "print(\"with smoothing score\",sentence_bleu([r1], candidate,smoothing_function=chencherry.method1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRB7DpCvPyHg",
        "outputId": "edee2b7b-86e0-45b9-eaa7-eb9dd6fbd346"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with smoothing score 0.6194291765462159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 谷歌翻译\n",
        "使用谷歌 Trax 进行翻译\n",
        "* 端到端的深度学习库\n",
        "* 包含一个可以用于机器翻译任务的transformer模型\n",
        "\n",
        "这里介绍（英语-德语问题）的最小功能"
      ],
      "metadata": {
        "id": "Qeaww3qERAkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Translation with Trax\n",
        "\n",
        "Note by Denis Rothman: The original notebook was split into cells.\n",
        "\n",
        "[Reference Code](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb)\n"
      ],
      "metadata": {
        "id": "dDgmJixeRudZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow==2.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1tkItTgHmKcn",
        "outputId": "e479ce3a-0914-4305-e064-12e3e2e4cd41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.62.2)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Installing collected packages: ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.2\n",
            "    Uninstalling ml-dtypes-0.3.2:\n",
            "      Successfully uninstalled ml-dtypes-0.3.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.3.3\n",
            "    Uninstalling keras-3.3.3:\n",
            "      Successfully uninstalled keras-3.3.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.16.2\n",
            "    Uninstalling tensorboard-2.16.2:\n",
            "      Successfully uninstalled tensorboard-2.16.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.16.1\n",
            "    Uninstalling tensorflow-2.16.1:\n",
            "      Successfully uninstalled tensorflow-2.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "9b2f487070d3405f9d63bfc7fcbb0c0f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installing Trax\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "!pip install -q -U trax\n",
        "import trax"
      ],
      "metadata": {
        "id": "EYZ9fzmlP5k9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating a tranformer model\n",
        "# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin\n",
        "model = trax.models.Transformer(\n",
        "    input_vocab_size=33300,\n",
        "    d_model=512, d_ff=2048,\n",
        "    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
        "    max_len=2048, mode='predict')\n"
      ],
      "metadata": {
        "id": "TJv4m-BQR2vN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initializing the model using pre-trained weights 预训练权重 初始化模型\n",
        "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n",
        "                     weights_only=True)"
      ],
      "metadata": {
        "id": "Fior6S46SHia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenizing a sentence 句子词元化\n",
        "sentence = 'I am only a machine but I have machine intelligence.'\n",
        "\n",
        "tokenized = list(trax.data.tokenize(iter([sentence]),  # Operates on streams.\n",
        "          vocab_dir='gs://trax-ml/vocabs/',\n",
        "          vocab_file='ende_32k.subword'))[0]\n",
        "tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dt_ngAIv5Dy",
        "outputId": "9c64dffd-9d6c-4b2e-d786-63bf9b06fe9f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  46,  131,  132,   13, 4435,  101,   46,   43, 4435, 7763,    3])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Decoding from the Transformer\n",
        "tokenized = tokenized[None, :]  # Add batch dimension.\n",
        "print(tokenized)\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results.\n",
        "print(tokenized_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iZuMBBGwClG",
        "outputId": "48021c7d-e733-4a7a-8084-4a78f40441b6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  46  131  132   13 4435  101   46   43 4435 7763    3]]\n",
            "[[  161   724   120    41 12770     5     2   163   104   531 12770 28153\n",
            "  22734     3     1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title De-tokenizing and Displaying the Translation\n",
        "tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\n",
        "print(tokenized_translation)\n",
        "translation = trax.data.detokenize(tokenized_translation,\n",
        "                   vocab_dir='gs://trax-ml/vocabs/',\n",
        "                   vocab_file='ende_32k.subword')\n",
        "print(\"The sentence:\",sentence)\n",
        "print(\"The translation:\",translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUQp328RwEPo",
        "outputId": "5f084949-df41-47c8-d3d8-483f5b366633"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  161   724   120    41 12770     5     2   163   104   531 12770 28153\n",
            " 22734     3]\n",
            "The sentence: I am only a machine but I have machine intelligence.\n",
            "The translation: Ich bin nur eine Maschine, aber ich habe Maschinenübersicht.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating a tranformer model / 改变 temperature，翻译结果变化\n",
        "# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin\n",
        "model = trax.models.Transformer(\n",
        "    input_vocab_size=33300,\n",
        "    d_model=512, d_ff=2048,\n",
        "    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
        "    max_len=2048, mode='predict')\n",
        "\n",
        "#@title Initializing the model using pre-trained weights 预训练权重 初始化模型\n",
        "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n",
        "                     weights_only=True)\n",
        "\n",
        "#@title Tokenizing a sentence 句子词元化\n",
        "sentence = 'I am only a machine but I have machine intelligence.'\n",
        "\n",
        "tokenized = list(trax.data.tokenize(iter([sentence]),  # Operates on streams.\n",
        "          vocab_dir='gs://trax-ml/vocabs/',\n",
        "          vocab_file='ende_32k.subword'))[0]\n",
        "print(tokenized)\n",
        "\n",
        "#@title Decoding from the Transformer\n",
        "tokenized = tokenized[None, :]  # Add batch dimension.\n",
        "print(tokenized)\n",
        "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, tokenized, temperature=1.0)  # Higher temperature: more diverse results.\n",
        "print(tokenized_translation)"
      ],
      "metadata": {
        "id": "wq9QEs87wHR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title De-tokenizing and Displaying the Translation\n",
        "tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\n",
        "print(tokenized_translation)\n",
        "translation = trax.data.detokenize(tokenized_translation,\n",
        "                   vocab_dir='gs://trax-ml/vocabs/',\n",
        "                   vocab_file='ende_32k.subword')\n",
        "print(\"The sentence:\",sentence)\n",
        "print(\"The translation:\",translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_GRncLLx48N",
        "outputId": "3c1912b3-a01a-4f3b-f835-2092c62dcead"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  161   724   120    88 12770     5     2   163   104   531 12770 16980\n",
            "  7712   300    12  1581 10797 27662     3]\n",
            "The sentence: I am only a machine but I have machine intelligence.\n",
            "The translation: Ich bin nur einer Maschine, aber ich habe Maschinenkontik und -zugriff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GntShPAKye05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}