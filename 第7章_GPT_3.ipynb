{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOE8bamdltwRTcGkcvC5bdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwh133/Transformer/blob/main/%E7%AC%AC7%E7%AB%A0_GPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   openAI gpt-3模型的训练包含17501个参数，在具有28500个CPU和10000个GPU的超级计算机上进行训练；\n",
        "*   零样本实验：指不对模型的参数进行微调，直接将预训练模型用于下游，只需要使用API即可执行未专门训练的任务\n",
        "\n"
      ],
      "metadata": {
        "id": "E3EbQz1XzEeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   GPT-2尚未达到基础模型的标准，基础模型不需要进行一步微调即可执行各种任务；\n",
        "* GPT-3可以\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1lNJj9buAAV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer模型从需要微调演化到零样本，**零样本的GPT-3 transformer模型不需要微调**"
      ],
      "metadata": {
        "id": "iT-G3BDxAkgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   GPT-3模型，【层数】从6层增加到**96层**；\n",
        "* GPT-3模型，【每层头数】从8个增加到**96个**\n",
        "*   GPT-3模型，【上下文大小】从512个词元增加到**12288个**\n",
        "\n"
      ],
      "metadata": {
        "id": "hYScQYnbBFrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 上下文大小和最长距离\n",
        "\n",
        "分析长期依赖关系所需的路径：从以前的 RNN、LSTM的循环层 转变为**注意力层**</br>\n",
        "自注意力将操作简化为一对一的词源操作\n",
        "\n",
        "\n",
        "\n",
        "*   RNN的循环层只能逐步存储上下文因此最长距离就是上下文大小\n",
        "*   RNN不能将上下文拆分为在并行机器架构上运行的96个头\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-yb5VZJYB0_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "目标:使用无标注数据训练transformer模型让注意力层通过无监督数据来学习语言</br>\n",
        "open AI训练transformer模型去学习语言,ta创建一个与任务无关的模型,使用原始数据来训练transformer模型而不是依赖专家标注的数据\n",
        "\n",
        "\n",
        "*   open AI 先使用无监督学习来训练transformer模型,然后使用监督学习对模型进行微调\n",
        "*   open AI 只使用原始transformer架构的解码器部分,效果不错\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D_wNYos6DrKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 四个阶段\n",
        "\n",
        "\n",
        "*   【微调】 先训练一个transformer模型,然后在下游任务上进行微调.——open AI团队逐步将微调任务数目减少到零\n",
        "*   【少样本】gpt训练好后，需要推理时根据任务给出的少量样本**作为条件进行推理**，这种条件推理取代微调方式的权重更新，gpt这种方式就**不需要微调**了\n",
        "* 【单样本】gpt训练好后，只需**一个样本作为条件进行推理**，而不需要微调更新权重\n",
        "* 【零样本】gpt训练好后，**不需要任何样本**即可执行下游任务\n",
        "\n"
      ],
      "metadata": {
        "id": "8EFWJQmvEvX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 预训练GPT-2 345M模型进行文本补全\n",
        "包括24个解码器层、16个头的自注意力子层\n",
        "\n"
      ],
      "metadata": {
        "id": "-3xKK8wWGNT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 2: Cloning the OpenAI GPT-2 Repository\n",
        "#!git clone https://github.com/openai/gpt-2.git March 2023 update,changed to:\n",
        "!git clone https://github.com/nshepperd/gpt-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxKspY_vHt7w",
        "outputId": "0ca608f8-d00c-486d-d2b4-acec31e4420d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 429, done.\u001b[K\n",
            "remote: Total 429 (delta 0), reused 0 (delta 0), pack-reused 429\u001b[K\n",
            "Receiving objects: 100% (429/429), 4.47 MiB | 7.58 MiB/s, done.\n",
            "Resolving deltas: 100% (218/218), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4 Checking the Version of TensorFlow\n",
        "#Colab has tf 1.x and tf 2.x installed\n",
        "#Restart runtime using 'Runtime' -> 'Restart runtime...'\n",
        "#%tensorflow_version 1.x March 2023 update\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VlfHtcsINjv",
        "outputId": "705f0e0b-ab49-4186-d63f-d5c8fc4f7bf8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5: Downloading the 345M parameter GPT-2 Model\n",
        "# run code and send argument\n",
        "import os # after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '345M'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taVmwE93H6qm",
        "outputId": "62caa69a-835e-42ec-98e7-a6ab7e78449e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 4.14Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.64Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 5.30Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:43, 32.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 18.4Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 2.41Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.74Mit/s]                                                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7: Project Source Code\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")"
      ],
      "metadata": {
        "id": "yWw0EFX_IoDy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7a: Interactive Conditional Samples (src)\n",
        "#Project Source Code for Interactive Conditional Samples:\n",
        "# /content/gpt-2/src/interactive_conditional_samples.py file\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf #March 2023 update"
      ],
      "metadata": {
        "id": "z_AcqfagI3kV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7b: Importing model sample encoder\n",
        "import model, sample, encoder\n",
        "#if following message:\n",
        "#ModuleNotFoundError: No module named 'tensorflow.contrib'\n",
        "#then go back and run Step 2 Checking TensorFlow version"
      ],
      "metadata": {
        "id": "LL8ZohMWHQ9T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8: Defining the model：def interact_model\n",
        "def interact_model(\n",
        "    model_name, # 模型的名称，用于加载特定的 GPT-2 模型变体，如 '345M'。\n",
        "    seed, # 生成随机数的种子，确保结果的可复现性\n",
        "    nsamples, # 生成文本-样本数量\n",
        "    batch_size, # 生成文本-批次大小\n",
        "    length, # 生成文本-长度\n",
        "    temperature, # 值越高生成的文本越随机\n",
        "    top_k, # 从概率最高的 k 个单词中选择下一个单词\n",
        "    models_dir # 模型文件的目录路径\n",
        "):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Zqw_cwsIHEq5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4Ycmrn1y9e-",
        "outputId": "2ae0151c-99a6-446f-8e0f-367dfe6213d5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================== SAMPLE 1 ========================================\n",
            " If we can talk them down 60 beats, right? But then we have to blame ourselves. We say, well, it's halfspeed.\" Rex Kalen.\"\n",
            "\n",
            "One day, I received a Spanish-speaking letter attached to my last receipt:   \"... much to my joy, a Spanish mandolin today!! Thank you.\"\"\n",
            "\n",
            "My hope, Tom Layton of Eddy Scott Studio, goes out to all sax fans around the world when it is been discovered that their favorite saxophonist lost his eye (heinz lies buried next to him after an ugly accident there). He is now resting in Seychelles, avoiding flying with the tranquillity of an elated pilgrim.OK, it was 90 with me when I last saw Tom Layton, he is aging wallflower with good taste, and is unexpectedly taken with Italian venezia fragrantaria tobacco from erupting Mysteria Barbieri. Like much of his later work, it sounds clean, powerful, and melodious like it is straight out of the Ornette Coleman stylebox. It sounds ecstatic beyond belief, a feeling that got absorbed by doors that ere there something wonderful between Totlinhood Pure Muchformas and Muro Ara. The combination of dissonant cant and transformational landscapes brings bacon and taquitos to Zeitgeist, And Marie Antoinette, with the backing of Rob Willis, The Cycles. Continue reading                                   \n",
            "\n",
            "Color Butler's donates\n",
            "================================================================================\n",
            "======================================== SAMPLE 1 ========================================\n",
            "I am a bad song.In the worldI dare stand in a sea.I wouldn't fall behind him.\"There's a song in the mountainI have always felt can't sing.I feel anywhere I look.There is a song sung on top.It goes.But she answers too well.There's only one meI can sing in my dulcet voice.I just want to sing wiselyI wish I could get well now.To this day, I miss it.But my own songy pathLife is a kind of countdown --But life can't spin me roundBut coming out of me a rayAnd I'm -You know you walked this way I knew this wayNo matter what happensJust keep moving and hopingNow just keep moving on my own wings.You can see on your own... I pick you up-- Don't stay close I'll quit nowI don't want to drag you off He never left meI knew couldn't fightWatching all the turnsJust find out I'm on my ownEarin', oh it doesn't last anymoreI sink into analogI know in ummm like many hopeNo one can mess with two sodding orange birdsEens of something, like to live.Then you see and blow off the low roofChorus - mamma don't tell them don't tell'Cause I'm old and wipedOut there morning doing wrongI turned my back Seein and then you hit it hard laterThere is a breath\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#@title Step 9: Interacting with GPT-2\n",
        "interact_model('345M',None,1,1,300,1,0,'/content/gpt-2/models')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 自定义数据集，微调GPT-2 117M模型进行文本补全\n",
        "包括12个解码器层、12个头的自注意力子层"
      ],
      "metadata": {
        "id": "5O12ZAI2GxHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 12: Interactive Context and Completion Examples 直接调用训练好的自定义gpt-2语言模型 进行文本补全\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name '117M'"
      ],
      "metadata": {
        "id": "P_8cs0CLLWoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 零样本GPT-3 进行文本补全(在线运行)\n",
        "https://platform.openai.com/playground"
      ],
      "metadata": {
        "id": "BoGo5FHOMfnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用 GPT-3 进行文本补全（API接口）"
      ],
      "metadata": {
        "id": "6d1IqjY7PjkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Installing & importing OpenAI"
      ],
      "metadata": {
        "id": "AhaxFcc_QQ-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai\n",
        "  import openai"
      ],
      "metadata": {
        "id": "KNgtkD1PMjrG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Entering the API KEY\n",
        "不支持给中国提供服务。。。。</br>\n",
        "示例源代码：https://beta.openai.com/examples</br>\n",
        "试验示例：https://platform.openai.com/playground"
      ],
      "metadata": {
        "id": "QSgnJ16zQZq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=\"[YOUR_KEY]\""
      ],
      "metadata": {
        "id": "tMSt0wvBQRxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] ='[YOUR_KEY or KEY variable]'\n",
        "print(os.getenv('OPENAI_API_KEY'))\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy4B4xEpQbi5",
        "outputId": "2beb905c-bf61-4871-ec43-76cdf046839f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[YOUR_KEY or KEY variable]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Running an NLP tasks with the default parameters\n",
        "### Step 4: Example 1: Grammar correction\n",
        "\n",
        "https://beta.openai.com/examples/default-grammar"
      ],
      "metadata": {
        "id": "XOw9YajiQoDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"Original: She no went to the market.\\nStandard American English:\",\n",
        "  temperature=0,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "\n",
        "#displaying the response object\n",
        "print(response)"
      ],
      "metadata": {
        "id": "47P16h9yQoXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning GPT-3/ 微调\n",
        "\n",
        "Copyright 2022 Denis Rothman\n",
        "\n",
        "[OpenAI fine-tuning documentation](https://beta.openai.com/docs/guides/fine-tuning/)\n",
        "\n"
      ],
      "metadata": {
        "id": "rN76UYHVWzdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Preparing the dataset"
      ],
      "metadata": {
        "id": "SkX-AoVyXhjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Installing OpenAI & Wandb\n",
        "\n",
        "Restart the runtime after installing openai and run the cell again to make sur that \"import openai\" is executed."
      ],
      "metadata": {
        "id": "D2nSAFF9XkoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai\n",
        "  import openai"
      ],
      "metadata": {
        "id": "9Oo3TrgkTfsC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Weights and Biases 【wandb可视化日志】\n",
        "\n",
        "\n",
        "\n",
        "Use W&B to build better models faster. Track and visualize all the pieces of your machine learning pipeline, from datasets to production models.\n",
        "\n",
        "Quickly identify model regressions. Use W&B to visualize results in real time, all in a central dashboard.\n",
        "Focus on the interesting ML. Spend less time manually tracking results in spreadsheets and text files.\n",
        "Capture dataset versions with W&B Artifacts to identify how changing data affects your resulting models.\n",
        "Reproduce any model, with saved code, hyperparameters, launch commands, input data, and resulting model weights.\n"
      ],
      "metadata": {
        "id": "p_SLcBOKXqen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import wandb\n",
        "except:\n",
        "  !pip install wandb\n",
        "  import wandb"
      ],
      "metadata": {
        "id": "Atas1Y6yXm6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Your API Key"
      ],
      "metadata": {
        "id": "XGspt2tyX8wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key=[YOUR_API_KEY]"
      ],
      "metadata": {
        "id": "22AELRX1XtFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Preparing the data\n",
        "\n",
        "Answer \"Y\" to all of the questions."
      ],
      "metadata": {
        "id": "i4EfTMgSYDLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f \"kantgpt.csv\""
      ],
      "metadata": {
        "id": "oAs9M8m-X8fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Fine-tuning ADA微调"
      ],
      "metadata": {
        "id": "QNkLsqa9YMvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Creating an OS environment for the API key"
      ],
      "metadata": {
        "id": "XeOHNlYXYSIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "print(os.getenv('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "JU2bcdMCYOaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Fine-tuning GPT-3 with the ADA engine\n",
        "\n",
        "When prompted choose and click on the \"ENTER\" button."
      ],
      "metadata": {
        "id": "L9Dik0imYTIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t \"kantgpt_prepared.jsonl\" -m \"ada\""
      ],
      "metadata": {
        "id": "R3IGnjhGYUtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Using the fine-tuned GPT-3 for a completion task\n",
        "\n",
        "Note: If your fine-tuned model does not appear immediately after the end of the fine-tuning process, you might have to wait until it is processed by OpenAI. You can also:\n",
        "\n",
        "1.go to the OpenAI Playground to test your model: https://platform.openai.com/playground\n",
        "\n",
        "2.select your model in the dropdown list and test it in that environment"
      ],
      "metadata": {
        "id": "lNHLQG90YWcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api completions.create -m ada:[YOUR_MODEL INFO] -p \"Several concepts are a priori such as\""
      ],
      "metadata": {
        "id": "nceyE1M7Yu1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3MqKQdLMYxfH"
      }
    }
  ]
}